
\paragraph{Sarsa($\lambda$)}
The agent is based on the Sarsa($\lambda$) algorithm, which learns a set of
action values $Q_t(s,a)$ by the use of eligibility
traces~\cite{Sutton:1998:IRL:551283}. The eligibility trace is initialized to
zero for every state-action pair, and for every transition all traces are
decayed by a factor $\gamma\lambda$. In addition, the trace of the current
state-action pair is incremented by $1$. The Q-table is then updated according
to:
\begin{equation}
        \forall s,a : Q_{t+1}(s,a) = Q_t(s,a) + \alpha\delta_t Z_t(s,a),
\end{equation}
where $\delta_t$ is the temporal difference defined by $\delta_t = R_{t+1} +
\gamma Q_t(S_{t+1},A_{t+1}) - Q_t(S_t,A_t)$. The Q-values are initialized to
$R_{max}$ for all small state space environments ($\vert{S}\vert < 50$) and to
$(R_{min}+R_{max})/2$ otherwise, since an optimistic initialization encourages
exploration among the set of actions.

\paragraph{KL-UCB}
Lorem ipsum dolor sit
amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et
dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco
laboris nisi ut aliquip ex ea commodo consequat. ~\cite{DBLP:journals/jmlr/GarivierC11}

\input import/maximiser.tex

