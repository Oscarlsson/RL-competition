
\paragraph{Sarsa($\lambda$)}
The agent is based on the Sarsa($\lambda$) algorithm, which learns a set of
action values $Q_t(s,a)$ by the use of eligibility
traces~\cite{Sutton:1998:IRL:551283}. The eligibility trace $Z_t(s,a)$ is
initialized to zero for every state-action pair, and for every transition all
traces are decayed by a factor $\gamma\lambda$. In addition, the trace of the
current state-action pair is incremented by $1$. The Q-table is then updated
according to:
\begin{equation}
        \forall s,a : Q_{t+1}(s,a) = Q_t(s,a) + \alpha\delta_t Z_t(s,a),
\end{equation}
where $\delta_t$ is the temporal difference defined by $\delta_t = R_{t+1} +
\gamma Q_t(S_{t+1},A_{t+1}) - Q_t(S_t,A_t)$. The Q-values are initialized to
$R_{max}$ for all small state space environments ($\vert{S}\vert < 50$) and to
$(R_{min}+R_{max})/2$ otherwise, since an optimistic initialization encourages
exploration among the set of actions.

\paragraph{KL-UCB}
An Upper-Confidence Bound (UCB) policy maps a state $s$ to the action maximizing
an upper bound. For Kullback-Leibler UCB, the bound for a given action $a$ is
defined by:
\begin{equation}
    \max_{q \in ]0,1[}\big\{n_s(a)
        D_{\mathrm{KL}}\left(\overline{R_s(a)}\Big\Vert{q}\right) \le \log(\tau) +
        c\log\log(\tau))\big\}
\end{equation}
where $D_\mathrm{KL}(P\Vert{Q}) = p \log \frac{p}{q}+(1-p)\log\frac{1-p}{1-q}$
is the Kullback-Leibler divergence measure of $p$ and
$q$.~\cite{garivier2011kl}

To combine the UCB methods with Sarsa($\lambda$) we let the average reward
$\overline{R_s(a)}=Q(s,a)$, and $n_s(a)=\sum_{i=1}^tZ_i(s,a)$ be a measure of
how often the state-action pair $(s,a)$ has been updated. Further, we let $\tau
= \sum_a{n_s(a)}$ denote a local time for the bandit problem in context $s$.

\input import/maximiser.tex

