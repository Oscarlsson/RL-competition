
\paragraph{Sarsa($\lambda$)}
The agent is based on the Sarsa($\lambda$) algorithm, which learns a set of
action values $q_t(s,a)$ by the use of eligibility
traces~\cite{Sutton:1998:IRL:551283}. For every step, the trace of all state
action pairs are decayed by a factor $\gamma\lambda$, and the trace of the
current state action pair is also increased by $1$. The Q-table is then updated
according to:
\begin{equation}
        Q_{t+1}(s,a) = Q_t(s,a) + \alpha\delta_t Z_t(s,a) \forall s,a,
\end{equation}
where $\delta_t$ is the temporal difference defined by $\delta_t = R_{t+1} +
\gamma Q_t(S_{t+1},A_{t+1}) - Q_t(S_t,A_t)$.

% INITIALIZATION OF Q-TABLE
% INITIALIZATION OF Q-TABLE
% INITIALIZATION OF Q-TABLE

\paragraph{KL-UCB}
Wanna know more?~\cite{DBLP:journals/jmlr/GarivierC11}Lorem ipsum dolor sit
amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et
dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco
laboris nisi ut aliquip ex ea commodo consequat. 

\paragraph{Tiebreaker}
Indeed we did.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do
eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo

