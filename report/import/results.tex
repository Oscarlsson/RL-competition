\paragraph{How we chose the maximiser}
We tried both to always choose
the highest valued action, and selecting an action with probability proportional
to (rescaled) value. This method of breaking ties in the $Q$ matrix proved useful
in board game environments  where the number of states were large. For the smaller
 environments, it didn't add any benefit and actually caused the agent to
perform worse in some examples. It replaced a simple "first best" choice, which turned out
to actually be a better decision in some cases, hence we did not chose to use the maximiser in these environments.The softer version of choosing proportional to
gain were less successful in all cases.

%Since the tie breakers, or maximisers, will not come into play for small systems more than a few times, we only use them for environments with large state-action spaces. Among the test environments, this constitutes the board game environments.

%We tested two versions of UCB policies in an attempt to shore up Sarsa($\lambda$)s unsuitability for bandit problems. The idea was to consider all states as independent bandit problems, where the UCB policies would decide the balance between exploration vs exploitation. The simpler UCB1 does not consider any upper bound on the reward and will never be satisfied, this causes it to start exploration even when faced with certain victory options. The KL-UCB considers the case where the reward is bounded. Both however, assumes long time horizons, where exploration occurs gradually. When faced with the short experiments in the competition, where the optimal approach should be to explore first, and exploit later, they fared worse than simply going for the best option. This is also due to that many of the environments have many large penalties and few rewards. Exploring anything after the first reward is found is not cost efficient. 

In the end we chose to only use the maximiser in environments with large state
space; e.g. tic-tac-toe and connect four. The maximiser is triggered more often in
environments with large state spaces since ties will occur more often.
Fig~\ref{fig:tiemaximiser} shows the slight increase in cumulative reward with
maximiser enabled compared to runs without maximiser.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{../data/tiebreakerplotconnectfourtictactoe.png}
    \caption{Comparing the use of maximiser on tic-tac-toe and connect four in a experiment with 100 episodes and reward averaged over 10 runs. }
    \label{fig:tiemaximiser}
\end{figure}
%When testing our agent on very different environments, from simple board games to context bandit problems. We observe that even very general approaches to improving performance against a large class of environments, might hurt the performance against other environments. We addressed large state space environments, where the state is altered gradually and good actions are good over a range of similar states, and introduced a tie breaker that tries to make informed decisions when faced with a lack of statistical data. This replaced a simple "first best" choice, which turned out to actually be a better decision in some cases. 
\paragraph{UCB policies}
We tested two versions of UCB policies in an attempt to shore up
Sarsa($\lambda$)'s unsuitability for bandit problems. The idea was to consider
all states as independent bandit problems, where the UCB policies would decide
the balance between exploration vs exploitation. The simpler UCB1 does not
consider any upper bound on the reward and will never be satisfied and this causes
it to perform exploration even when faced with certain victory options. The KL-UCB
considers the case where the reward is bounded. Both however, assumes long time
horizons, where exploration occurs gradually. When faced with the short
experiments in the competition, where the optimal approach should be to explore
first, and exploit later, they fared worse than simply going for the best
option. We believe the meagre success with exploration is also due to that many 
of the environments have many large penalties and few rewards. Exploring anything 
but the first found reward is not cost efficient. The fact that all states are considered a new
case where exploration should be performed only amplifies the cost of using UCB.

\paragraph{Performance}
The parameter space for our agent is very large, $\lambda$ decides how much we
diffuse the reward, step size $\alpha$ determines how quick the agent overwrites
old information, how we initialise the Q matrix determines if the agent is
optimistic or pessimistic when it comes to the degree of exploration. With the
UCB policies, we had the option to adjust the tendency to explore with a scalar,
we have different maximisers that attempt to exploit structure in the problems.
Naturally we didn't have time to sweep all parameters for all environments, but
we varied different parameters, keeping most constant and tried to find good
parameter values for different types of problems. At initialisation, the agent
uses known information of a problem to set the agent parameters, e.g. enables
the maximiser for large state spaces, under the assumption that the test
environments are good representations of the competition environments.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{../data/lambdasweepplot.png}
    \caption{How the cumulative reward changes with the different values of $\lambda$.}
    \label{fig:lambdasweep}
\end{figure}
The performance of our agent is visualized in Figure~\ref{fig:cumreward}. It shows how the agents average performance over 65 runs
in a 100 episode experiment against all seven competition environments. Maximiser is off, no UCB and $Q$ is initialised to max reward, i.e. optimistic explorative. Tic tac toe benefit greatly from $\lambda= 0$, mines perform slightly better with $\lambda$ around 0.2. After the competition we noticed that the chain environment performed noticeably better for high $\lambda$. The other environments doesn't depend strong enough on $\lambda$ to warrant any specific value. Since we lacked sufficient chains data, we decided on $\lambda = 0.2$ for unknown environments.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{../data/100episodes_50runs.png}
    \caption{Agents cumulative reward averaged over 50 runs in a experiment with 100 episodes}
    \label{fig:cumreward}
\end{figure} 
The yellow curve represents the performance on the mines environment and at around 10 episodes it is clear that the agent has learned the environment. The performance on Connect four, represented as the red curve
in Fig~\ref{fig:cumreward}, has a much lower maximum reward than the other
environments, which explains why it might look like the agent performs worse in this environment, but the agent does indeed learn this environment too.

Fig~\ref{fig:mineslearning} shows how our agent gradually finds a shorter (but not the shortest) path to the goal in the mines environment. Red coloured paths are early in the experiment, green are later. In this case, from the 18th episode and onwards, the agent takes the same path every time.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{../data/minPlot.png}
    \caption{Example of how the agent learns a short path from start to goal (yellow). The agent traverse the world 100 times, early attempts are coloured red, later green. Hitting a mine (black) costs a penalty. Some noise is added to the paths to make it easier to distinguish them.}
    \label{fig:mineslearning}
\end{figure} 

We also noticed that Sarsa($\lambda$) in its original formulation will decide on the next action before it updates the Q matrix. In the specific case where penalties are incurred for illegal moves, and the agents are forced to try again, this causes the agent to perform every illegal move twice. We decided to stick with the original, suboptimal version, since checking policy twice at each time step is inelegant and we wanted to avoid overtraining.

All code, plots and anything other related to this project is publicly available via ~\cite{githubrepo}.
